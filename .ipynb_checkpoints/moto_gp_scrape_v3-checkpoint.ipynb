{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for all the data we will be scraping in this notebook\n",
    "headers = ['Year','TRK','Track','Category','Session','Date','Track_Condition','Track_Temp','Air_Temp',\n",
    "           'Humidity','Position','Points','Rider_Number','Rider_Name','Nationality','Team_Name',\n",
    "           'Bike','Avg_Speed','Time']\n",
    "#had to remove 2002 - 2004\n",
    "years = ['2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "\n",
    "base_url = 'http://www.motogp.com/en/Results+Statistics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_stuff(url):\n",
    "    \"\"\"Returns a BeautifulSoup object for the provided url\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(soup):\n",
    "    \"\"\" Returns the date of the race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='padbot5')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = ','.join(find.text.replace(',',' ').split()[-3:])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_con(soup):\n",
    "    \"\"\" Returns the track condition during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather track_condition')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[2]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_tmp(soup):\n",
    "    \"\"\" Returns the track temperature during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather ground')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[1]\n",
    "    return r\n",
    "\n",
    "def get_air_tmp(soup):\n",
    "    \"\"\" Returns the air temperature during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather air')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[1]\n",
    "    return r\n",
    "\n",
    "def get_humidity(soup):\n",
    "    \"\"\" Returns the track humidity during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather humidity')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[1]\n",
    "    return r\n",
    "\n",
    "def get_all_races(soup):\n",
    "    \"\"\" Returns all the races that took place in a particular season\n",
    "        for which the soup was passed in \"\"\"\n",
    "    find = soup.find(id='event')\n",
    "    if find is None:\n",
    "        r = []\n",
    "    else:\n",
    "        r = find.find_all('option')\n",
    "    return r\n",
    "\n",
    "def get_all_cats(soup):\n",
    "    \"\"\" Returns all the different categories (MotoGP, Moto2, etc.)\n",
    "        that took place at a particular track in the provided soup \"\"\"\n",
    "    find = soup.find(id='category')\n",
    "    if find is None:\n",
    "        r = []\n",
    "    else:\n",
    "        r = find.find_all('option')\n",
    "    return r\n",
    "\n",
    "def get_race_sessions(soup):\n",
    "    \"\"\" Returns all the different race sessions (RACE, RACE2, etc.)\n",
    "        that took place at a particular track in the provided soup \"\"\"\n",
    "    find = soup.find(id='session')\n",
    "    r = []\n",
    "    if find is None:\n",
    "        return r\n",
    "    else:\n",
    "        r2 = find.find_all('option')\n",
    "        for s in r2:\n",
    "            if s.text.find('RACE') > -1:\n",
    "                r.append(s.text.replace('E',''))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_stats(soup, year, trk, track, cat, ssn):\n",
    "    \n",
    "    if soup.find('tbody') is None:\n",
    "        return [dict(zip(headers, [year, trk, track, cat, ssn]+['n/a']*(len(headers)-3)))]\n",
    "    else:\n",
    "        riders = soup.find('tbody').find_all('a')\n",
    "        stats_to_return = []\n",
    "\n",
    "        # raceday stats\n",
    "        date = get_date(soup)\n",
    "        tr_con = get_tr_con(soup)\n",
    "        tr_tmp = get_tr_tmp(soup)\n",
    "        air_tmp = get_air_tmp(soup)\n",
    "        humid = get_humidity(soup)\n",
    "        \n",
    "        # rider stats\n",
    "        for r in riders:\n",
    "            pos = r.findPrevious().findPrevious().findPrevious().findPrevious().text\n",
    "            if pos=='':\n",
    "                pos='crash'\n",
    "            else:\n",
    "                pos=int(pos)    \n",
    "            points = r.findPrevious().findPrevious().findPrevious().text\n",
    "            if points=='':\n",
    "                points=0\n",
    "            else:\n",
    "                points=float(points)\n",
    "            r_num = r.findPrevious().findPrevious().text\n",
    "            if r_num != '':\n",
    "                r_num = int(r_num)\n",
    "            r_nam = r.text\n",
    "            r_nat = r.findNext().text\n",
    "            team = r.findNext().findNext().text\n",
    "            bike = r.findNext().findNext().findNext().text\n",
    "            avgspd = r.findNext().findNext().findNext().findNext().text\n",
    "            time = r.findNext().findNext().findNext().findNext().findNext().text\n",
    "\n",
    "            stats_dict = dict(zip(headers, [year, trk, track, cat, ssn, date, tr_con, tr_tmp, air_tmp,\n",
    "                                            humid, pos, points, r_num, r_nam, r_nat, team,\n",
    "                                            bike, avgspd, time]))\n",
    "            stats_to_return.append(stats_dict)\n",
    "\n",
    "        return stats_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "QAT, ARG, AME, SPA, FRA, ITA, CAT, NED, GER, CZE, AUT, GBR, RSM, ARA, THA, JPN, AUS, MAL, VAL, 2017\n",
      "QAT, ARG, AME, SPA, FRA, ITA, CAT, NED, GER, CZE, AUT, GBR, RSM, ARA, JPN, AUS, MAL, VAL, 2016\n",
      "QAT, ARG, AME, SPA, FRA, ITA, CAT, NED, GER, AUT, CZE, GBR, RSM, ARA, JPN, AUS, MAL, VAL, 2015\n",
      "QAT, AME, ARG, SPA, FRA, ITA, CAT, NED, GER, INP, CZE, GBR, RSM, ARA, JPN, AUS, MAL, VAL, 2014\n",
      "QAT, AME, ARG, SPA, FRA, ITA, CAT, NED, GER, INP, CZE, GBR, RSM, ARA, JPN, AUS, MAL, VAL, 2013\n",
      "QAT, AME, SPA, FRA, ITA, CAT, NED, GER, USA, INP, CZE, GBR, RSM, ARA, MAL, AUS, JPN, VAL, 2012\n",
      "QAT, SPA, POR, FRA, CAT, GBR, NED, GER, ITA, USA, INP, CZE, RSM, ARA, JPN, MAL, AUS, VAL, 2011\n",
      "QAT, SPA, POR, FRA, CAT, GBR, NED, ITA, GER, USA, CZE, INP, RSM, ARA, JPN, AUS, MAL, VAL, 2010\n",
      "QAT, SPA, FRA, ITA, GBR, NED, CAT, GER, USA, CZE, INP, RSM, ARA, JPN, MAL, AUS, POR, VAL, 2009\n",
      "QAT, JPN, SPA, FRA, ITA, CAT, NED, USA, GER, GBR, CZE, INP, RSM, POR, AUS, MAL, VAL, 2008\n",
      "QAT, SPA, POR, CHN, FRA, ITA, CAT, GBR, NED, GER, USA, CZE, RSM, INP, JPN, AUS, MAL, VAL, 2007\n",
      "QAT, SPA, TUR, CHN, FRA, ITA, CAT, GBR, NED, GER, USA, CZE, RSM, POR, JPN, AUS, MAL, VAL, 2006\n",
      "SPA, QAT, TUR, CHN, FRA, ITA, CAT, NED, GBR, GER, USA, CZE, MAL, AUS, JPN, POR, VAL, 2005\n",
      "SPA, POR, CHN, FRA, ITA, CAT, NED, USA, GBR, GER, CZE, JPN, MAL, QAT, AUS, TUR, VAL, 2004\n",
      "RSA, SPA, FRA, ITA, CAT, NED, RIO, GER, GBR, CZE, POR, JPN, QAT, MAL, AUS, VAL, 2003\n",
      "JPN, RSA, SPA, FRA, ITA, CAT, NED, GBR, GER, CZE, POR, RIO, PAC, MAL, AUS, VAL, 2002\n",
      "JPN, RSA, SPA, FRA, ITA, CAT, NED, GBR, GER, CZE, POR, RIO, PAC, MAL, AUS, VAL, >> Scraping complete!\n"
     ]
    }
   ],
   "source": [
    "# loop through all parameters\n",
    "\n",
    "for yr in reversed(years):\n",
    "    data_list = []\n",
    "    soup_yr = soup_stuff(base_url + yr)\n",
    "    races = get_all_races(soup_yr)\n",
    "    print(yr)\n",
    "    \n",
    "    for rc in races:\n",
    "        TRK = rc['value']\n",
    "        Track = rc['title']\n",
    "        print(TRK, end=\", \")\n",
    "        url_rc = base_url +yr +'/' +TRK +'/'\n",
    "        soup_rc = soup_stuff(url_rc)\n",
    "        categories = get_all_cats(soup_rc)\n",
    "        \n",
    "        for cat in categories:\n",
    "            CAT = cat.text\n",
    "            url_c = base_url +yr +'/' +TRK +'/' + CAT + '/'\n",
    "            soup_c = soup_stuff(url_c)\n",
    "            sessions = get_race_sessions(soup_c)\n",
    "            \n",
    "            for ssn in sessions:\n",
    "                SSN = ssn\n",
    "                url_ssn = base_url +yr +'/' +TRK +'/' + CAT + '/' + SSN + '/Classification'\n",
    "                soup_ssn = soup_stuff(url_ssn)\n",
    "                data_list.extend(get_all_stats(soup_ssn, yr, TRK, Track, CAT, SSN))\n",
    "                time.sleep(1+np.random.random())\n",
    "    \n",
    "    df = pd.DataFrame(data_list, columns=headers)\n",
    "    #fn = '/Archive/' + yr + '_data.csv'\n",
    "    df.to_csv(\"race_results.csv\")\n",
    "    #print(fn)\n",
    "    time.sleep(1+np.random.random())\n",
    "\n",
    "print('>> Scraping complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018 - QAT, ARG, AME, SPA, FRA, ITA, CAT, NED, GER, CZE, AUT, GBR, RSM, ARA, THA, JPN, AUS, MAL, VAL, \n",
      "2017 - QAT, ARG, AME, SPA, FRA, ITA, CAT, NED, GER, CZE, AUT, GBR, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2016 - QAT, ARG, AME, SPA, FRA, ITA, CAT, NED, GER, AUT, CZE, GBR, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2015 - QAT, AME, ARG, SPA, FRA, ITA, CAT, NED, GER, INP, CZE, GBR, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2014 - QAT, AME, ARG, SPA, FRA, ITA, CAT, NED, GER, INP, CZE, GBR, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2013 - QAT, AME, SPA, FRA, ITA, CAT, NED, GER, USA, INP, CZE, GBR, RSM, ARA, MAL, AUS, JPN, VAL, \n",
      "2012 - QAT, SPA, POR, FRA, CAT, GBR, NED, GER, ITA, USA, INP, CZE, RSM, ARA, JPN, MAL, AUS, VAL, \n",
      "2011 - QAT, SPA, POR, FRA, CAT, GBR, NED, ITA, GER, USA, CZE, INP, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2010 - QAT, SPA, FRA, ITA, GBR, NED, CAT, GER, USA, CZE, INP, RSM, ARA, JPN, MAL, AUS, POR, VAL, \n",
      "2009 - QAT, JPN, SPA, FRA, ITA, CAT, NED, USA, GER, GBR, CZE, INP, RSM, POR, AUS, MAL, VAL, \n",
      "2008 - QAT, SPA, POR, CHN, FRA, ITA, CAT, GBR, NED, GER, USA, CZE, RSM, INP, JPN, AUS, MAL, VAL, \n",
      "2007 - QAT, SPA, TUR, CHN, FRA, ITA, CAT, GBR, NED, GER, USA, CZE, RSM, POR, JPN, AUS, MAL, VAL, \n",
      "2006 - SPA, QAT, TUR, CHN, FRA, ITA, CAT, NED, GBR, GER, USA, CZE, MAL, AUS, JPN, POR, VAL, \n",
      "2005 - SPA, POR, CHN, FRA, ITA, CAT, NED, USA, GBR, GER, CZE, JPN, MAL, QAT, AUS, TUR, VAL, \n",
      "2004 - RSA, SPA, FRA, ITA, CAT, NED, RIO, GER, GBR, CZE, POR, JPN, QAT, MAL, AUS, VAL, \n",
      "2003 - JPN, RSA, SPA, FRA, ITA, CAT, NED, GBR, GER, CZE, POR, RIO, PAC, MAL, AUS, VAL, \n",
      "2002 - JPN, RSA, SPA, FRA, ITA, CAT, NED, GBR, GER, CZE, POR, RIO, PAC, MAL, AUS, VAL, "
     ]
    }
   ],
   "source": [
    "# first, get all tracks from 2002-2018\n",
    "track_list = []\n",
    "GPs_list = []\n",
    "track_names = []\n",
    "\n",
    "for yr in reversed(years):\n",
    "    soup_yr = soup_stuff(base_url + yr)\n",
    "    races = get_all_races(soup_yr)\n",
    "    print('')\n",
    "    print(yr, end = \" - \")\n",
    "    \n",
    "    for rc in races:\n",
    "        TRK = rc['value']\n",
    "        Track = rc['title']\n",
    "        print(TRK, end=\", \")\n",
    "        track_list.append(TRK)\n",
    "        GPs_list.append(Track.split(' - ')[0])\n",
    "        track_names.append(Track.split(' - ')[1])\n",
    "        \n",
    "    time.sleep(1+np.random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the unique ones \n",
    "combined_list = []\n",
    "for index, item in enumerate(track_list):\n",
    "    combined_list.append(item+' - '+track_names[index])\n",
    "combined_track_set = set(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_track_set = combined_track_set.remove(\"JPN - Suzuka Circuit\")\n",
    "#combined_track_set = combined_track_set.remove(\"RSA - Phakisa Freeway\")\n",
    "#combined_track_set = combined_track_set.remove(\"ITA - Autodromo Internazionale del Mugello':'Italy\")\n",
    "combined_track_set = combined_track_set.remove(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually inserting tracks that lack a URL \n",
    "\n",
    "track_url_dict = {'AME - Circuit Of The Americas':'Americas',\n",
    "                  'ARA - MotorLand Aragón':'Aragon',\n",
    "                  'ARG - Termas de Río Hondo':'Argentina',\n",
    "                  'AUS - Phillip Island':'Australia',\n",
    "                  'AUT - Red Bull Ring – Spielberg':'Austria',\n",
    "                  'CAT - Circuit de Barcelona-Catalunya':'Catalunya',\n",
    "                  'CHN - Shanghai Circuit':0,\n",
    "                  'CZE - Automotodrom Brno':'Czech+Republic',\n",
    "                  'FRA - Le Mans':'France',\n",
    "                  'GBR - Donington Park Circuit':0,\n",
    "                  'GBR - Silverstone Circuit':'Great+Britain',\n",
    "                  'GER - Sachsenring':'Germany',\n",
    "                  'INP - Indianapolis Motor Speedway':0,\n",
    "                  'ITA - Autodromo del Mugello':'Italy',\n",
    "                  'JPN - Twin Ring Motegi':'Japan',\n",
    "                  'MAL - Sepang International Circuit':'Malaysia',\n",
    "                  'NED - TT Circuit Assen':'Netherlands',\n",
    "                  'POR - Estoril Circuit':0,\n",
    "                  'QAT - Losail International Circuit':'Qatar',\n",
    "                  'RSM - Misano World Circuit Marco Simoncelli':'San+Marino',\n",
    "                  'SPA - Circuito de Jerez':'Spain',\n",
    "                  'TUR - Istanbul Circuit':0,\n",
    "                  'USA - Mazda Raceway Laguna Seca':0,\n",
    "                  'VAL - Circuit Ricardo Tormo':'Valencia'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get basic track info\n",
    "def get_GP_info(track_url_str):\n",
    "    \"\"\"\n",
    "    Returns a list with track length, number of left corners, number of right corners,\n",
    "    track width, and length of longest straight. For any unavailable values, it returns\n",
    "    'n/a' instead of a float or int.\n",
    "    \"\"\"\n",
    "    url = 'http://www.motogp.com/en/event/' + track_url_str + '#info-track'\n",
    "    soupy = soup_stuff(url)\n",
    "    attributes = soupy.find(id='circuit_numbers').find_all(class_='circuit_number_content')\n",
    "    strs = []\n",
    "    list_data = []\n",
    "    \n",
    "    for s in range(len(attributes)):\n",
    "        strs.append(attributes[s].text)\n",
    "\n",
    "    if float(strs[0].split()[0])==0:\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(float(strs[0].split()[0]))\n",
    "\n",
    "    if strs[1]=='':\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(int(strs[1]))\n",
    "    \n",
    "    if strs[2]=='':\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(int(strs[2]))\n",
    "    \n",
    "    if len(strs[3].split())==1:\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(float(strs[3].split()[0]))\n",
    "    \n",
    "    if len(strs[4].split())==1:\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(float(strs[4].split()[0]))\n",
    "\n",
    "    return list_data\n",
    "\n",
    "def get_GP_info_additional(track_url_str):\n",
    "    \"\"\"\n",
    "    Returns MotoGP average speed, MotoGP distance, Moto2 distance,\n",
    "    and Moto3 distance for the particular track. If data does not exist,\n",
    "    it returns 'n/a' in place of a float or int.\n",
    "    \"\"\"\n",
    "    url = 'http://www.motogp.com/en/event/' + track_url_str + '#info-track'\n",
    "    soupy = soup_stuff(url)\n",
    "    \n",
    "    # MotoGP average speed\n",
    "    avg_speed_str = soupy.find(class_='c-statistics__speed-item').text\n",
    "    if avg_speed_str == '-' or avg_speed_str == '':\n",
    "        avg_speed = 'n/a'\n",
    "    else:\n",
    "        avg_speed = float(avg_speed_str)\n",
    "    \n",
    "    attributes = soupy.find(class_='c-event__row-item col-xs-12 col-lg-7 col-lg-pull-5').find_all(class_='c-laps__item')\n",
    "    \n",
    "    # MotoGP distance\n",
    "    GP_dist = attributes[1].text.split()[0]\n",
    "    GP_dist = float(GP_dist)\n",
    "    if GP_dist==0: GP_dist='n/a'\n",
    "        \n",
    "     # Moto2 distance\n",
    "    m2_dist = attributes[4].text.split()[0]\n",
    "    m2_dist = float(m2_dist)    \n",
    "    if m2_dist==0: m2_dist='n/a'\n",
    "        \n",
    "     # Moto3 distance\n",
    "    m3_dist = attributes[7].text.split()[0]\n",
    "    m3_dist = float(m3_dist)   \n",
    "    if m3_dist==0: m3_dist='n/a'\n",
    "        \n",
    "    return [avg_speed, GP_dist, m2_dist, m3_dist]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'THA - Chang International Circuit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-64b61f669ac1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrack_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrack\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcombined_track_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mtrack_url_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0ml_GP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_GP_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_url_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'THA - Chang International Circuit'"
     ]
    }
   ],
   "source": [
    "# make a list of dictionaries for track information\n",
    "headers_2 = ['GP','track_length_km','l_corners','r_corners',\n",
    "           'width_m','straight_m','GP_avg_speed','gp_dist',\n",
    "           'm2_dist','m3_dist']\n",
    "\n",
    "track_data = []\n",
    "for track in combined_track_set:\n",
    "    if track_url_dict[track] != 0:\n",
    "        print('//', end='')\n",
    "        l_GP, L_c, R_c, wid, strt = get_GP_info(track_url_dict[track])\n",
    "        GP_avg_spd, gp_d, m2_d, m3_d = get_GP_info_additional(track_url_dict[track])\n",
    "        track_dict = dict(zip(headers, [track,l_GP,L_c,R_c,wid,strt,GP_avg_spd,gp_d,m2_d,m3_d]))\n",
    "        track_data.append(track_dict)\n",
    "        time.sleep(1+np.random.random())\n",
    "print('Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.4\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.motogp.com/en/event/Qatar'\n",
    "soupy = soup_stuff(url)\n",
    "attributes = soupy.find(class_='c-event__row-item col-xs-12 col-lg-7 col-lg-pull-5').find_all(class_='c-laps__item')\n",
    "GP_dist = float(attributes[1].text.split()[0])\n",
    "if GP_dist==0: GP_dist='n/a'\n",
    "print(GP_dist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_track_set.remove('AUS - Phillip Island')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_track_set.remove('AUT - Red Bull Ring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_track_set.remove('RIO - Nelson Piquet Circuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
