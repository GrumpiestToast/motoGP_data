{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for all the data we will be scraping in this notebook\n",
    "headers = ['Year','TRK','Track','Category','Session','Date','Track_Condition','Track_Temp','Air_Temp',\n",
    "           'Humidity','Position','Points','Rider_Number','Rider_Name','Nationality','Team_Name',\n",
    "           'Bike','Avg_Speed','Time']\n",
    "headers2 = ['Track', \"Trk Length\", 'Left_Corners', 'Right_Corners', 'track_width', 'length of longest straight', 'MotoGP_avg_speed', 'GP_distance', 'Moto2_distance', 'Moto3_distance'] \n",
    "\n",
    "\n",
    "#had to remove 2002 - 2004\n",
    "years = ['2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "\n",
    "base_url = 'http://www.motogp.com/en/Results+Statistics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_stuff(url):\n",
    "    \"\"\"Returns a BeautifulSoup object for the provided url\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(soup):\n",
    "    \"\"\" Returns the date of the race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='padbot5')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = ','.join(find.text.replace(',',' ').split()[-3:])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_con(soup):\n",
    "    \"\"\" Returns the track condition during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather track_condition')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[2]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_tmp(soup):\n",
    "    \"\"\" Returns the track temperature during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather ground')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[1]\n",
    "    return r\n",
    "\n",
    "def get_air_tmp(soup):\n",
    "    \"\"\" Returns the air temperature during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather air')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[1]\n",
    "    return r\n",
    "\n",
    "def get_humidity(soup):\n",
    "    \"\"\" Returns the track humidity during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather humidity')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[1]\n",
    "    return r\n",
    "\n",
    "def get_all_races(soup):\n",
    "    \"\"\" Returns all the races that took place in a particular season\n",
    "        for which the soup was passed in \"\"\"\n",
    "    find = soup.find(id='event')\n",
    "    if find is None:\n",
    "        r = []\n",
    "    else:\n",
    "        r = find.find_all('option')\n",
    "    return r\n",
    "\n",
    "def get_all_cats(soup):\n",
    "    \"\"\" Returns all the different categories (MotoGP, Moto2, etc.)\n",
    "        that took place at a particular track in the provided soup \"\"\"\n",
    "    find = soup.find(id='category')\n",
    "    if find is None:\n",
    "        r = []\n",
    "    else:\n",
    "        r = find.find_all('option')\n",
    "    return r\n",
    "\n",
    "def get_race_sessions(soup):\n",
    "    \"\"\" Returns all the different race sessions (RACE, RACE2, etc.)\n",
    "        that took place at a particular track in the provided soup \"\"\"\n",
    "    find = soup.find(id='session')\n",
    "    r = []\n",
    "    if find is None:\n",
    "        return r\n",
    "    else:\n",
    "        r2 = find.find_all('option')\n",
    "        for s in r2:\n",
    "            if s.text.find('RACE') > -1:\n",
    "                r.append(s.text.replace('E',''))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_stats(soup, year, trk, track, cat, ssn):\n",
    "    \n",
    "    if soup.find('tbody') is None:\n",
    "        return [dict(zip(headers, [year, trk, track, cat, ssn]+['n/a']*(len(headers)-3)))]\n",
    "    else:\n",
    "        riders = soup.find('tbody').find_all('a')\n",
    "        stats_to_return = []\n",
    "\n",
    "        # raceday stats\n",
    "        date = get_date(soup)\n",
    "        tr_con = get_tr_con(soup)\n",
    "        tr_tmp = get_tr_tmp(soup)\n",
    "        air_tmp = get_air_tmp(soup)\n",
    "        humid = get_humidity(soup)\n",
    "        \n",
    "        # rider stats\n",
    "        for r in riders:\n",
    "            pos = r.findPrevious().findPrevious().findPrevious().findPrevious().text\n",
    "            if pos=='':\n",
    "                pos='crash'\n",
    "            else:\n",
    "                pos=int(pos)    \n",
    "            points = r.findPrevious().findPrevious().findPrevious().text\n",
    "            if points=='':\n",
    "                points=0\n",
    "            else:\n",
    "                points=float(points)\n",
    "            r_num = r.findPrevious().findPrevious().text\n",
    "            if r_num != '':\n",
    "                r_num = int(r_num)\n",
    "            r_nam = r.text\n",
    "            r_nat = r.findNext().text\n",
    "            team = r.findNext().findNext().text\n",
    "            bike = r.findNext().findNext().findNext().text\n",
    "            avgspd = r.findNext().findNext().findNext().findNext().text\n",
    "            time = r.findNext().findNext().findNext().findNext().findNext().text\n",
    "\n",
    "            stats_dict = dict(zip(headers, [year, trk, track, cat, ssn, date, tr_con, tr_tmp, air_tmp,\n",
    "                                            humid, pos, points, r_num, r_nam, r_nat, team,\n",
    "                                            bike, avgspd, time]))\n",
    "            stats_to_return.append(stats_dict)\n",
    "\n",
    "        return stats_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "QAT, ARG, AME, SPA, FRA, ITA, CAT, NED, GER, CZE, AUT, GBR, RSM, ARA, THA, JPN, AUS, MAL, VAL, "
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Archive/2018_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8bcdfdc2473d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/Archive/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0myr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 1745\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    155\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Archive/2018_data.csv'"
     ]
    }
   ],
   "source": [
    "# loop through all parameters\n",
    "\n",
    "for yr in reversed(years):\n",
    "    data_list = []\n",
    "    soup_yr = soup_stuff(base_url + yr)\n",
    "    races = get_all_races(soup_yr)\n",
    "    print(yr)\n",
    "    \n",
    "    for rc in races:\n",
    "        TRK = rc['value']\n",
    "        Track = rc['title']\n",
    "        print(TRK, end=\", \")\n",
    "        url_rc = base_url +yr +'/' +TRK +'/'\n",
    "        soup_rc = soup_stuff(url_rc)\n",
    "        categories = get_all_cats(soup_rc)\n",
    "        \n",
    "        for cat in categories:\n",
    "            CAT = cat.text\n",
    "            url_c = base_url +yr +'/' +TRK +'/' + CAT + '/'\n",
    "            soup_c = soup_stuff(url_c)\n",
    "            sessions = get_race_sessions(soup_c)\n",
    "            \n",
    "            for ssn in sessions:\n",
    "                SSN = ssn\n",
    "                url_ssn = base_url +yr +'/' +TRK +'/' + CAT + '/' + SSN + '/Classification'\n",
    "                soup_ssn = soup_stuff(url_ssn)\n",
    "                data_list.extend(get_all_stats(soup_ssn, yr, TRK, Track, CAT, SSN))\n",
    "                time.sleep(1+np.random.random())\n",
    "    \n",
    "    df = pd.DataFrame(data_list, columns=headers)\n",
    "    fn = '/Archive/' + yr + '_data.csv'\n",
    "    df.to_csv(fn)\n",
    "    print(fn)\n",
    "    time.sleep(1+np.random.random())\n",
    "\n",
    "print('>> Scraping complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, get all tracks from 2002-2018\n",
    "track_list = []\n",
    "GPs_list = []\n",
    "track_names = []\n",
    "\n",
    "for yr in reversed(years):\n",
    "    soup_yr = soup_stuff(base_url + yr)\n",
    "    races = get_all_races(soup_yr)\n",
    "    print('')\n",
    "    print(yr, end = \" - \")\n",
    "    \n",
    "    for rc in races:\n",
    "        TRK = rc['value']\n",
    "        Track = rc['title']\n",
    "        print(TRK, end=\", \")\n",
    "        track_list.append(TRK)\n",
    "        GPs_list.append(Track.split(' - ')[0])\n",
    "        track_names.append(Track.split(' - ')[1])\n",
    "        \n",
    "    time.sleep(1+np.random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the unique ones \n",
    "combined_list = []\n",
    "for index, item in enumerate(track_list):\n",
    "    combined_list.append(item+' - '+track_names[index])\n",
    "combined_track_set = set(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_track_set = combined_track_set.remove(\"JPN - Suzuka Circuit\")\n",
    "combined_track_set = combined_track_set.remove(\"RSA - Phakisa Freeway\")\n",
    "combined_track_set = combined_track_set.remove(\"ITA - Autodromo Internazionale del Mugello':'Italy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_track_set.remove('AUS - Phillip Island')\n",
    "combined_track_set.remove('AUT - Red Bull Ring')\n",
    "combined_track_set.remove('RIO - Nelson Piquet Circuit')\n",
    "combined_track_set.remove('THA - Chang International Circuit')\n",
    "combined_track_set.remove('PAC - Twin Ring Motegi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually inserting tracks that lack a URL \n",
    "\n",
    "track_url_dict = {'AME - Circuit Of The Americas':'Americas',\n",
    "                  'ARA - MotorLand Aragón':'Aragon',\n",
    "                  'ARG - Termas de Río Hondo':'Argentina',\n",
    "                  'AUS - Phillip Island':'Australia',\n",
    "                  'AUT - Red Bull Ring – Spielberg':'Austria',\n",
    "                  'CAT - Circuit de Barcelona-Catalunya':'Catalunya',\n",
    "                  'CHN - Shanghai Circuit':0,\n",
    "                  'CZE - Automotodrom Brno':'Czech+Republic',\n",
    "                  'FRA - Le Mans':'France',\n",
    "                  'GBR - Donington Park Circuit':0,\n",
    "                  'GBR - Silverstone Circuit':'Great+Britain',\n",
    "                  'GER - Sachsenring':'Germany',\n",
    "                  'INP - Indianapolis Motor Speedway':0,\n",
    "                  'ITA - Autodromo del Mugello':'Italy',\n",
    "                  'JPN - Twin Ring Motegi':'Japan',\n",
    "                  'MAL - Sepang International Circuit':'Malaysia',\n",
    "                  'NED - TT Circuit Assen':'Netherlands',\n",
    "                  'THA - Chang International Circuit' : 'Thailand',\n",
    "                  'POR - Estoril Circuit':0,\n",
    "                  'QAT - Losail International Circuit':'Qatar',\n",
    "                  'RIO - Nelson Piquet Circuit': 0,\n",
    "                  'RSM - Misano World Circuit Marco Simoncelli':'San+Marino',\n",
    "                  'SPA - Circuito de Jerez':'Spain',\n",
    "                  'TUR - Istanbul Circuit':0,\n",
    "                  'USA - Mazda Raceway Laguna Seca':0,\n",
    "                  'VAL - Circuit Ricardo Tormo':'Valencia'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get basic track info\n",
    "def get_GP_info(track_url_str):\n",
    "    \"\"\"\n",
    "    Returns a list with track length, number of left corners, number of right corners,\n",
    "    track width, and length of longest straight. For any unavailable values, it returns\n",
    "    'n/a' instead of a float or int.\n",
    "    \"\"\"\n",
    "    url = 'http://www.motogp.com/en/event/' + track_url_str + '#info-track'\n",
    "    soupy = soup_stuff(url)\n",
    "    attributes = soupy.find(id='circuit_numbers').find_all(class_='circuit_number_content')\n",
    "    strs = []\n",
    "    list_data = []\n",
    "    \n",
    "    for s in range(len(attributes)):\n",
    "        strs.append(attributes[s].text)\n",
    "\n",
    "    if float(strs[0].split()[0])==0:\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(float(strs[0].split()[0]))\n",
    "\n",
    "    if strs[1]=='':\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(int(strs[1]))\n",
    "    \n",
    "    if strs[2]=='':\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(int(strs[2]))\n",
    "    \n",
    "    if len(strs[3].split())==1:\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(float(strs[3].split()[0]))\n",
    "    \n",
    "    if len(strs[4].split())==1:\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(float(strs[4].split()[0]))\n",
    "\n",
    "    return list_data\n",
    "\n",
    "def get_GP_info_additional(track_url_str):\n",
    "    \"\"\"\n",
    "    Returns MotoGP average speed, MotoGP distance, Moto2 distance,\n",
    "    and Moto3 distance for the particular track. If data does not exist,\n",
    "    it returns 'n/a' in place of a float or int.\n",
    "    \"\"\"\n",
    "    url = 'http://www.motogp.com/en/event/' + track_url_str + '#info-track'\n",
    "    soupy = soup_stuff(url)\n",
    "    \n",
    "    # MotoGP average speed\n",
    "    avg_speed_str = soupy.find(class_='c-statistics__speed-item').text\n",
    "    if avg_speed_str == '-' or avg_speed_str == '':\n",
    "        avg_speed = 'n/a'\n",
    "    else:\n",
    "        avg_speed = float(avg_speed_str)\n",
    "    \n",
    "    attributes = soupy.find(class_='c-event__row-item col-xs-12 col-lg-7 col-lg-pull-5').find_all(class_='c-laps__item')\n",
    "    \n",
    "    # MotoGP distance\n",
    "    GP_dist = attributes[1].text.split()[0]\n",
    "    GP_dist = float(GP_dist)\n",
    "    if GP_dist==0: GP_dist='n/a'\n",
    "        \n",
    "     # Moto2 distance\n",
    "    m2_dist = attributes[4].text.split()[0]\n",
    "    m2_dist = float(m2_dist)    \n",
    "    if m2_dist==0: m2_dist='n/a'\n",
    "        \n",
    "     # Moto3 distance\n",
    "    m3_dist = attributes[7].text.split()[0]\n",
    "    m3_dist = float(m3_dist)   \n",
    "    if m3_dist==0: m3_dist='n/a'\n",
    "        \n",
    "    return [avg_speed, GP_dist, m2_dist, m3_dist]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a list of dictionaries for track information\n",
    "headers_2 = ['GP','track_length_km','l_corners','r_corners',\n",
    "           'width_m','straight_m','GP_avg_speed','gp_dist',\n",
    "           'm2_dist','m3_dist']\n",
    "\n",
    "track_data = []\n",
    "for track in combined_track_set:\n",
    "    if track_url_dict[track] != 0:\n",
    "        print('//', end='')\n",
    "        l_GP, L_c, R_c, wid, strt = get_GP_info(track_url_dict[track])\n",
    "        GP_avg_spd, gp_d, m2_d, m3_d = get_GP_info_additional(track_url_dict[track])\n",
    "        track_dict = dict(zip(headers2, [track,l_GP,L_c,R_c,wid,strt,GP_avg_spd,gp_d,m2_d,m3_d]))\n",
    "        track_data.append(track_dict)\n",
    "        time.sleep(1+np.random.random())\n",
    "print('Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://www.motogp.com/en/event/Qatar'\n",
    "# soupy = soup_stuff(url)\n",
    "# attributes = soupy.find(class_='c-event__row-item col-xs-12 col-lg-7 col-lg-pull-5').find_all(class_='c-laps__item')\n",
    "# GP_dist = float(attributes[1].text.split()[0])\n",
    "# if GP_dist==0: GP_dist='n/a'\n",
    "# print(GP_dist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually add in the info for tracks which have a 0 in the track_url_dict\n",
    "# information is from archived PDFs like the one at this following link\n",
    "# http://resources.motogp.com/files/results/2006/CHN/circuit+information.pdf?v1_96143780\n",
    "\n",
    "\n",
    "dict_austria = {'Track' : 'AUT - Red Bull Ring', 'MotoGP_avg_speed' : 182.8, 'GP_distance' : 120.9, \n",
    "                'Left_Corners' : 3.0, 'Moto2_distance' : 108.0, ' Moto3_distance' : 99.3, 'Right_Corners' : 7.0, \n",
    "                'length of longest straight' : 626.0, 'Trk Length':4.3, 'track_width':13.0}\n",
    "\n",
    "dict_shanghai = {'Track' : 'CHN - Shanghai Circuit', 'MotoGP_avg_speed' : 'n/a', 'GP_distance' : 'n/a', \n",
    "                'Left_Corners' : 7.0, 'Moto2_distance' : 'n/a', ' Moto3_distance' : 'n/a', 'Right_Corners' : 7.0, \n",
    "                'length of longest straight' : 1202.0, 'Trk Length':5.281, 'track_width':14.0}\n",
    "\n",
    "dict_donington = {'Track' : 'GBR - Donington Park Circuit', 'MotoGP_avg_speed' : 'n/a', 'GP_distance' : 'n/a', \n",
    "                'Left_Corners' : 4.0, 'Moto2_distance' : 'n/a', ' Moto3_distance' : 'n/a', 'Right_Corners' : 7.0, \n",
    "                'length of longest straight' : 564.0, 'Trk Length':4.023, 'track_width':10.0}\n",
    "\n",
    "dict_indianapolis = {'Track' : 'INP - Indianapolis Motor Speedway', 'MotoGP_avg_speed' : 'n/a', 'GP_distance' : 'n/a', \n",
    "                'Left_Corners' : 10.0, 'Moto2_distance' : 'n/a', ' Moto3_distance' : 'n/a', 'Right_Corners' : 6.0, \n",
    "                'length of longest straight' : 644.0, 'Trk Length':4.216, 'track_width':16.0}\n",
    "\n",
    "dict_estoril = {'Track' : 'POR - Estoril Circuitt', 'MotoGP_avg_speed' : 'n/a', 'GP_distance' : 'n/a', \n",
    "                'Left_Corners' : 4.0, 'Moto2_distance' : 'n/a', ' Moto3_distance' : 'n/a', 'Right_Corners' : 9.0, \n",
    "                'length of longest straight' : 986.0, 'Trk Length':4.182, 'track_width':14.0}\n",
    "\n",
    "dict_istanbul = {'Track' : 'TUR - Istanbul Circuit', 'MotoGP_avg_speed' : 'n/a', 'GP_distance' : 'n/a', \n",
    "                'Left_Corners' : 8.0, 'Moto2_distance' : 'n/a', ' Moto3_distance' : 'n/a', 'Right_Corners' : 6.0, \n",
    "                'length of longest straight' : 720.0, 'Trk Length':5.340, 'track_width':21.0}\n",
    "\n",
    "dict_laguna = {'Track' : 'USA - Mazda Raceway Laguna Seca', 'MotoGP_avg_speed' : 'n/a', 'GP_distance' : 'n/a', \n",
    "                'Left_Corners' : 7.0, 'Moto2_distance' : 'n/a', ' Moto3_distance' : 'n/a', 'Right_Corners' : 4.0, \n",
    "                'length of longest straight' : 966.0, 'Trk Length':3.610, 'track_width':15.0}\n",
    "\n",
    "dict_australia = {'Track' : 'AUS - Phillip Island', 'MotoGP_avg_speed' : 176.3, 'GP_distance' : 120.1, \n",
    "                'Left_Corners' : 7.0, 'Moto2_distance' : 111.2, ' Moto3_distance' : 102.3, 'Right_Corners' : 5.0, \n",
    "                'length of longest straight' : 900.0, 'Trk Length':4.4, 'track_width':13.0}\n",
    "\n",
    "dict_brazil = {'Track' : 'RIO - Autódromo Internacional Nelson Piquet', 'MotoGP_avg_speed' : 'n/a', 'GP_distance' : 'n/a', \n",
    "                'Left_Corners' : 6.0 , 'Moto2_distance' : 'n/a', ' Moto3_distance' : 'n/a', 'Right_Corners' : 6.0, \n",
    "                'length of longest straight' : 'n/a', 'Trk Length': 5.031, 'track_width':'n/a'}\n",
    "\n",
    "dict_Thai = {'Track' : 'THA - Chang International Circuit', 'MotoGP_avg_speed' : 177.9, 'GP_distance' : 118.4, \n",
    "                #'Left_Corners' : 5.0, 'Moto2_distance' : 109.3, ' Moto3_distance' : 100.2, 'Right_Corners' : 7.0, \n",
    "                #'length of longest straight' : 1000.0, 'Trk Length':4.6, 'track_width':12.0}\n",
    "\n",
    "dict_motegi = {'Track' : 'JPN - Twin Ring Motegi', 'MotoGP_avg_speed' : 162.2, 'GP_distance' : 115.2, \n",
    "                'Left_Corners' : 6.0, 'Moto2_distance' : 105.6, ' Moto3_distance' : 96.0, 'Right_Corners' : 8.0, \n",
    "                'length of longest straight' : 762.0, 'Trk Length':4.8, 'track_width':15.0}\n",
    "\n",
    "dist_italy = {'Track' : 'ITA - Autodromo del Mugello', 'MotoGP_avg_speed' : 174.2, 'GP_distance' : 120.6, \n",
    "                'Left_Corners' : 6.0, 'Moto2_distance' : 110.1, ' Moto3_distance' : 104.9, 'Right_Corners' : 9, \n",
    "                'length of longest straight' : 1141.0, 'Trk Length':5.2, 'track_width':14.0}\n",
    "\n",
    "\n",
    "\n",
    "track_data.append(dict_shanghai)\n",
    "track_data.append(dict_donington)\n",
    "track_data.append(dict_indianapolis)\n",
    "track_data.append(dict_estoril)\n",
    "track_data.append(dict_istanbul)\n",
    "track_data.append(dict_laguna)\n",
    "track_data.append(dict_australia)\n",
    "track_data.append(dict_brazil)\n",
    "track_data.append(dict_Thai)\n",
    "track_data.append(dict_motegi)\n",
    "track_data.append(dict_austria)\n",
    "track_data.append(dict_italy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe from new data\n",
    "df_tracks = pd.DataFrame(track_data, columns=headers2)\n",
    "df_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'Racetrack_data.csv'\n",
    "df_tracks.to_csv(fn)\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df_new =pd.DataFrame(track_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df_new.drop_duplicates(\"Track\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
